#0.21 0.75 0.38 0.22 1.01 0.40
#0.30 1.78 0.50 0.31 3.70 0.54
#0.38 0.22 0.86 0.39 0.22 1.02
#0.47 0.30 2.85 0.52 0.32 4.00
# This table, based on Raftery and Zeh (1991) represents the time in h
# that certain whales need to cover 1 km. Is true that the distribution of the
# variables is aproximately simetric? Are 80% of the observations higher than the
# mean?Is true the 95% are placed between the mean and two sd?
whales = '0.16 0.16 0.17 0.19 0.20 0.21 0.22 0.22 0.23 0.25 0.25 0.28 0.33 0.34
0.34 0.34 0.35 0.35 0.42 0.45 0.45 0.45 0.45 0.45 0.55 0.63 0.68 0.71 0.74 0.75
1.14 1.26 1.29 1.31 1.36 1.42 0.21 0.75 0.38 0.22 1.01 0.40 0.30 1.78 0.50 0.31
3.70 0.54 0.38 0.22 0.86 0.39 0.22 1.02 0.47 0.30 2.85 0.52 0.32 4.00'
whales_split = as.double(unlist(strsplit(whales, " ")))
print(whales_split)
summary(whales_split)
mean_whales = mean(whales_split)
median_whales = median(whales_split)
sd_whales = sd(whales_split)
hist(whales_split, breaks=c(seq(from = 0, to = 4, by=0.1)))
ecdf(mean_whales)
percentile <- ecdf(whales_split)
percentile(mean_whales)
#No, the distribution of the variables is not approximately symmetric
quantile(whales_split, probs=c(seq(from = 0, to = 1, by=0.05)))
#No, since the mean is 0.679 and the 20th percentile is equal to 0.228
percentile((mean_whales + 2*sd_whales))
#Yes this is true
## Exercise Four. File 'heads.csv' contains data with the diameter in cm of a
# certain group of people. Write an script that caclulates the statistics that
# we saw in the lectures. Remove all the observations with less than 13 cm.
heads_original = read.csv('/Users/benseimon/Downloads/heads.csv')
heads_working = data.frame(heads_original)
summary(heads_working)
heads_greater_13 = subset(heads_working, diameter>=13)
## Exercise Five. Load the built in data eurodist. You must check how to load
# built-in data in R. Look for the description of the data. Use the command
# as.matrix to work with this data. What is the farthest city to Barcelona?
data(eurodist)
eurodist_matrix = as.matrix(eurodist)
Barcelona = eurodist_matrix[,2]
print(Barcelona)
Barcelona[match(max(Barcelona), Barcelona)]
hist(whales_split, breaks=c(seq(from = 0, to = 4, by=0.1), main='Distribution of
required time for whales to cover 1km', xlab='time taken'))
hist(whales_split, breaks=c(seq(from = 0, to = 4, by=0.1)), main='Distribution of
required time for whales to cover 1km', xlab='time taken')
install.packages('babynames')
install.packages('DAAG')
install.packages('datasets')
install.packages('directlabels')
install.packages('faraway')
install.packages("datasets")
install.packages("datasets")
install.packages("datasets")
install.packages("datasets")
install.packages("datasets")
install.packages("datasets")
install.packages('directlabels')
install.packages('faraway')
install.packages('ggmap')
install.packages('ggradar')
install.packages('igraph')
install.packages('lattice')
install.packages('leaflet')
install.packages('MASS')
install.packages('mgcv')
install.packages('nutshell')
install.packages('plotly')
install.packages('RColorBrewer')
install.packages('readr')
install.packages('tidyverse')
install.packages('tidyr')
#adjacency matrix for network from Q2
adjacency_matrix = matrix(
c(0,1,1,0,0,1,1,1,1,
1,0,0,0,0,0,0,0,0,
1,0,0,1,0,1,0,0,0,
0,0,1,0,0,1,0,0,0,
0,0,0,0,0,1,0,0,0,
1,0,1,1,1,0,0,0,0,
1,0,0,0,0,0,0,0,0,
1,0,0,0,0,0,0,0,0,
1,0,0,0,0,0,0,0,0),
nrow = 9,
ncol = 9)
#z = c(1,1,2,2,2,2,1,1,1)
n = 9
c = 3
z <- sample(1:c, n, replace = TRUE)
f = c(0,1,0,0,0,0,0,0,0)
#make graph
graph_q2 <- graph_from_adjacency_matrix(
adjacency_matrix,
mode = c("undirected"),
weighted = NULL,
)
log_likelihood_function <- function(graph, communities_list){
log_likelihood <- c()
for (i in seq.int(length(unique(membership(communities_list))))){
m <- (ecount(graph))
kappa <- sum(degree(graph, communities_list[[i]]))
each_count <- ecount(induced_subgraph(graph, communities_list[[i]]))
if (each_count != 0){
first_term <- each_count/(2*m)
second_term <- log(first_term/((kappa/(2*m))*(kappa/(2*m))))
log_likelihood <- c(log_likelihood, ((first_term*second_term)))}
else {}
#print(paste0(i, 'has', ecount(induced_subgraph(graph_q2, communities_list[[i]]))))
}
remaining_combos = combinations(length(unique(membership(communities_list))), 2)
for (x in seq.int(1, length(remaining_combos)/2)){
index1 = remaining_combos[x, 1]
index2 = remaining_combos[x, 2]
m <- (ecount(graph))
kappa1 <- sum(degree(graph, communities_list[[index1]]))
kappa2 <- sum(degree(graph, communities_list[[index2]]))
each_count <- length(E(graph)[communities_list[[index1]] %--% communities_list[[index2]]])
if (each_count != 0){
first_term <- each_count/(2*m)
second_term <- log((first_term/((kappa1/(2*m))*(kappa2/(2*m)))))
log_likelihood <- c(log_likelihood, ((first_term*second_term)))}
else {}
#print(paste0(index1, index2, 'has', length(E(graph) [communities_list[[index1]] %--% communities_list[[index2]]])))
results <- sum(log_likelihood)
}
return(results)
}
makeAMove <- function(graph, current_partition, groups, isFrozen){
log_each_node <- c()
move_each_node <- c()
final_results <- c()
z_list <- c()
#get the current allocation of communities
communities <- make_clusters(graph, current_partition)
input_log <- log_likelihood_function(graph, communities)
#get the index of the first node in the current partition
for (node in seq.int(1, length(current_partition))){
#if the node has not been moved
if (isFrozen[node] == 0){
#loop over number of communities
for (group in seq.int(1, c)){
#ignore the community that the node is already in
if (current_partition[node] != group){
#for the remaining potential communities, compute the log-likelihood of each combination
#make a copy of the input community structure
new_partition = current_partition
#change the group the node is in
new_partition[node] = group
#set the new community structure
new_communities <- make_clusters(graph, new_partition)
#function that computes the log likelihood for a given graph and community structure
log_likelihood <- log_likelihood_function(graph, new_communities)
log_each_node <- c(log_each_node, log_likelihood)
node_new_group <- c(node, new_partition[node])
move_each_node = c(move_each_node, list(node_new_group))
z_list = append(z_list, list(new_partition))
}
}}}
#get the index of the max log-likelihood
index_best_log = which.max(log_each_node)
#retrieve best log-likelihood and the move
best_log = log_each_node[index_best_log]
best_move = move_each_node[index_best_log]
#plot original community structure
#plot(communities, graph, main = paste0("Log-likelihood:", input_log))
#get best communities and plot
best_partition <- z_list[[index_best_log]]
best_communities <- make_clusters(graph, best_partition)
#plot(best_communities, graph, main = paste0("Log-likelihood:", best_log))
#create final results
final_results = c(final_results, list(best_log, best_move))
return(final_results)
}
#comm <- make_clusters(graph_q2, z)
#plot(comm, graph_q2)
#test <- log_likelihood_function(graph_q2, comm)
#adjacency matrix for network from Q2
adjacency_matrix = matrix(
c(0,1,1,0,0,1,1,1,1,
1,0,0,0,0,0,0,0,0,
1,0,0,1,0,1,0,0,0,
0,0,1,0,0,1,0,0,0,
0,0,0,0,0,1,0,0,0,
1,0,1,1,1,0,0,0,0,
1,0,0,0,0,0,0,0,0,
1,0,0,0,0,0,0,0,0,
1,0,0,0,0,0,0,0,0),
nrow = 9,
ncol = 9)
#z = c(1,1,2,2,2,2,1,1,1)
n = 9
c = 3
z <- sample(1:c, n, replace = TRUE)
f = c(0,1,0,0,0,0,0,0,0)
#make graph
graph_q2 <- graph_from_adjacency_matrix(
adjacency_matrix,
mode = c("undirected"),
weighted = NULL,
)
runOnePhase <- function(graph, initial_partition, groups){
final_output <- c()
halt <- c()
best_log_likelihood_list <- c()
best_partition_list <- c()
#get communities for log-likelihood function
original_communities <- make_clusters(graph, initial_partition)
initial_log_likelihood = log_likelihood_function(graph, original_communities)
isFrozen <- sample(0, n, replace = TRUE)
#stop when all nodes are frozen and call
while(sum(isFrozen) < length(isFrozen)){
for (node in seq.int(1, length(initial_partition))){
#if the node has not been moved
if (isFrozen[node] == 0){
best_move_results <- makeAMove(graph, initial_partition, groups, isFrozen)
#find the node that moved and freeze
index_moved_node = best_move_results[[2]][[1]][1]
group_moved_node = best_move_results[[2]][[1]][2]
isFrozen[index_moved_node] <- 1
#print(index_moved_node)
#print(isFrozen)
best_log_likelihood = c(best_move_results[[1]])
new_partition = c(initial_partition)
new_partition[index_moved_node] = group_moved_node
best_log_likelihood_list <- c(best_log_likelihood_list, list(best_log_likelihood))
best_partition_list <- c(best_partition_list, list(new_partition))
}}
index_best_log = which.max(best_log_likelihood_list)
best_log = best_log_likelihood_list[index_best_log]
if (best_log > initial_log_likelihood){
best_partition_z_star = best_partition_list[index_best_log]
binary_halt = 0}
else if (best_log <= initial_log_likelihood){
best_partition_z_star = initial_partition
binary_halt = 1
}}
final_output <- list(best_partition_z_star, best_log, binary_halt, best_log_likelihood_list)
return(final_output)
}
fitDCSBM <- function(graph, groups, number_of_phases){
halt <- 0
phase_best_partitions <- c()
phase_best_log_likelihoods <- c()
phase_all_log_likelihoods <- c()
initial_partition <- sample(1:groups, n, replace = TRUE)
running_phase_count <- 0 #keeping track of the phases
#get communities for log-likelihood function
original_communities <- make_clusters(graph, initial_partition)
initial_log_likelihood = log_likelihood_function(graph, original_communities)
#print(initial_log_likelihood)
#don't stop until halt is TRUE and the phases run is less than the parameter for phases
for (phase in seq.int(1, number_of_phases)){
if (running_phase_count <= number_of_phases){
#print(initial_partition)
phase_results <- runOnePhase(graph, initial_partition, groups)
running_phase_count <- running_phase_count + 1
if (halt == 0){
if(phase_results[[2]][[1]] > initial_log_likelihood){
phase_best_ll <- phase_results[[2]][[1]]
phase_best_part <- phase_results[[1]][[1]]
phase_all_ll <- phase_results[[4]]
phase_best_log_likelihoods <- c(phase_best_log_likelihoods, list(phase_best_ll))
phase_best_partitions <- c(phase_best_partitions, list(phase_best_part))
phase_all_log_likelihoods <- c(phase_all_log_likelihoods, phase_all_ll)
initial_partition <- phase_best_part
initial_log_likelihood <- phase_best_ll
#print(phase_best_ll)
}
else if (phase_results[[2]][[1]] <= initial_log_likelihood){
phase_best_ll <- phase_results[[2]][[1]]
phase_best_part <- initial_partition
phase_all_ll <- phase_results[[4]]
phase_best_log_likelihoods <- c(phase_best_log_likelihoods, list(phase_best_ll))
phase_best_partitions <- c(phase_best_partitions, list(phase_best_part))
phase_all_log_likelihoods <- c(phase_all_log_likelihoods, phase_all_ll)
halt <- 1
}}
else if (halt == 1){
break
}
}}
fit_results <- list(phase_best_log_likelihoods, phase_best_partitions, running_phase_count-1, phase_all_log_likelihoods)
return(fit_results)
}
test_results <- makeAMove(graph_q2, z, c, f)
test_one_phase <- runOnePhase(graph_q2, z, c)
test_fit <- fitDCSBM(graph_q2, c, 30)
library(igraphdata)
data(karate)
n = vcount(karate)
c = 2
z <- sample(1:c, n, replace = TRUE)
karate_fit <- fitDCSBM(karate, 2, 30)
library(igraphdata)
data(karate)
repeat_fit <- function(graph, groups, number_of_phases, repetitions){
list_of_ll <- c()
list_of_partitions <- c()
best_partition <- c()
max_ll <- c()
#use the fit function
for (i in seq.int(1, length(repetitions))){
fit_results <- fitDCSBM(graph, groups, number_of_phases)
for (i in seq.int(1, length(fit_results[[1]]))){
ll <- c(fit_results[[1]][[i]])
partition <- c(fit_results[[2]][[i]])
list_of_ll <- c(list_of_ll, list(ll))
list_of_partitions <- c(list_of_partitions, list(partition))
}
}
index_max <- which.max(list_of_ll)
max_ll <- c(max_ll, list_of_ll[[index_max]])
best_partition <- c(best_partition, list_of_partitions[[index_max]])
results_repeat_fit <- list(max_ll, best_partition)
best_communities <- make_clusters(graph, best_partition)
plot(best_communities, graph)
return(results_repeat_fit)
}
karate_repeat_fit <- repeat_fit(karate, 2, n, 5000)
require(gdistance)
install.packages(gdistance)
require(gdistance)
install.packages(gdistance)
install.packages('gdistance')
# Input Files
# Note: the alternate, 'walking_only' friction surface is named friction_surface_2019_v51_walking_only.tif
friction.surface.filename <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Problem Sets/Problem Set 2/R_data/2020_motorized_friction_surface.geotiff'
point.filename <- 'E:\\accessibility\\points.csv' # Just 2 columns.  Structured as [X_COORD, Y_COORD] aka [LONG, LAT].  Use a header.
# Output Files
T.filename <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Problem Sets/Problem Set 2/R_outputs/study.area.T.rds'
T.GC.filename <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Problem Sets/Problem Set 2/R_outputs/study.area.T.GC.rds'
output.filename <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Problem Sets/Problem Set 2/R_outputs/study.area.accessibility.tif'
library("ggplot2", quietly = TRUE)
library("dplyr", quietly = TRUE)
class <- mpg %>%
group_by(class) %>%
summarise(n = n(), hwy = mean(hwy))
head(class)
install.packages("remotes")
remotes::install_github("cranedroesch/panelNNET")
install.packages("remotes")
# load some packages
library(chron)
library(ncdf4)
library(lattice)
library(RColorBrewer)
# set path and filename
ncpath <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Term Paper/climate/historical/hurs'
ncname <- "gfdl-esm4_r1i1p1f1_w5e5_historical_hurs_mwi_daily_1981_1990"
ncfname <- paste(ncpath, ncname, ".nc", sep="")
dname <- "hurs"  # note: tmp means temperature (not temporary)
install.packages('chron')
# load some packages
library(chron)
library(ncdf4)
library(lattice)
library(RColorBrewer)
# set path and filename
ncpath <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Term Paper/climate/historical/hurs'
ncname <- "gfdl-esm4_r1i1p1f1_w5e5_historical_hurs_mwi_daily_1981_1990"
ncfname <- paste(ncpath, ncname, ".nc", sep="")
dname <- "hurs"  # note: tmp means temperature (not temporary)
ncin <- nc_open(ncfname)
# load some packages
library(chron)
library(ncdf4)
library(lattice)
library(RColorBrewer)
# set path and filename
ncpath <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Term Paper/climate/historical/hurs/'
ncname <- "gfdl-esm4_r1i1p1f1_w5e5_historical_hurs_mwi_daily_1981_1990"
ncfname <- paste(ncpath, ncname, ".nc", sep="")
dname <- "hurs"  # note: tmp means temperature (not temporary)
# open a netCDF file
ncin <- nc_open(ncfname)
print(ncin)
lon <- ncvar_get(ncin,"lon")
nlon <- dim(lon)
head(lon)
lat <- ncvar_get(ncin,"lat")
nlat <- dim(lat)
head(lat)
print(c(nlon,nlat))
time <- ncvar_get(ncin,"time")
time
tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits
var_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(var_array)
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])
chron(time,origin=c(tmonth, tday, tyear))
# replace netCDF fill values with NA's
tmp_array[tmp_array==fillvalue$value] <- NA
# replace netCDF fill values with NA's
var_array[var_array==fillvalue$value] <- NA
# replace netCDF fill values with NA's
var_array[var_array==fillvalue$value] <- NA
memory.limit()
library(chron)
library(ncdf4)
library(lattice)
library(RColorBrewer)
# set path and filename
ncpath <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Term Paper/climate/historical/hurs/'
ncname <- "gfdl-esm4_r1i1p1f1_w5e5_historical_hurs_mwi_daily_1981_1990"
ncfname <- paste(ncpath, ncname, ".nc", sep="")
dname <- "hurs"  # note: tmp means temperature (not temporary)
# open a netCDF file
ncin <- nc_open(ncfname)
print(ncin)
# get longitude and latitude
lon <- ncvar_get(ncin,"lon")
nlon <- dim(lon)
head(lon)
lat <- ncvar_get(ncin,"lat")
nlat <- dim(lat)
head(lat)
print(c(nlon,nlat))
# get time
time <- ncvar_get(ncin,"time")
time
tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits
# get variable
var_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(var_array)
# get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")
# convert time -- split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])
chron(time,origin=c(tmonth, tday, tyear))
# replace netCDF fill values with NA's
var_array[var_array==fillvalue$value] <- NA
library(chron)
library(ncdf4)
library(lattice)
library(RColorBrewer)
# set path and filename
ncpath <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Term Paper/climate/historical/hurs/'
ncname <- "gfdl-esm4_r1i1p1f1_w5e5_historical_hurs_mwi_daily_1981_1990"
ncfname <- paste(ncpath, ncname, ".nc", sep="")
dname <- "hurs"  # note: tmp means temperature (not temporary)
library(chron)
library(ncdf4)
library(lattice)
library(RColorBrewer)
# set path and filename
ncpath <- '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/Spatial Data/Term Paper/climate/historical/hurs/'
ncname <- "gfdl-esm4_r1i1p1f1_w5e5_historical_hurs_mwi_daily_1981_1990"
ncfname <- paste(ncpath, ncname, ".nc", sep="")
dname <- "hurs"  # note: tmp means temperature (not temporary)
# open a netCDF file
ncin <- nc_open(ncfname)
print(ncin)
# get longitude and latitude
lon <- ncvar_get(ncin,"lon")
nlon <- dim(lon)
head(lon)
lat <- ncvar_get(ncin,"lat")
nlat <- dim(lat)
head(lat)
print(c(nlon,nlat))
# get time
time <- ncvar_get(ncin,"time")
time
tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits
# get variable
var_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(var_array)
# get variable
var_array <- ncvar_get(ncin,dname)
library("devtools")
install_github("sachaepskamp/psychonetrics")
library("psychonetrics")
library("dplyr")
setwd('/Users/benseimon/Documents/Barca GSE/Studies/Term 3/Advanced Techniques/term_paper/psych-networks-brownlees/')
Data <- read.csv("Supplementary2_data.csv")
